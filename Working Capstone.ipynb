{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn. __version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_rows', 2000)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_review = pd.read_pickle('reviews_all.pkl')\n",
    "# df_review['review_body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_main = pd.read_pickle('df_main.pkl')\n",
    "df_main.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    " \n",
    "\n",
    "s = df_main['vitolas']\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "df_vitola = pd.DataFrame(mlb.fit_transform(s),columns= (f'vitola_' + mlb.classes_), index=df_main.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_vitola\n",
    "#del df_vitola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_vitola = pd.concat([df_main, df_vitola], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_KNN = df_main_vitola.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_KNN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_KNN1 = df_KNN.drop(['vitolas', 'Shapes','prod_name', 'rating', 'rating_count', 'desc'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF to Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_KNN1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_KNN1.to_pickle('KNN1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "imputer = IterativeImputer(ExtraTreesRegressor())\n",
    "inv_tran = []\n",
    "# create a list of categorical columns to iterate over\n",
    "cat_cols = ['profile','Binder','Filler','Flavored','Has Tip','Origin','Pressed','Sweet','Wrapper', 'Brand:']\n",
    "\n",
    "def encode(data):\n",
    "    '''function to encode non-null data and replace it in the original data'''\n",
    "    #retains only non-null values\n",
    "    nonulls = np.array(data.dropna())\n",
    "    #reshapes the data for encoding\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    #encode date\n",
    "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
    "    #Encoder Dict\n",
    "    inverse_trans = inv_tran.append(encoder.categories_)\n",
    "    #Assign back encoded values to non-null values\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    return data\n",
    "\n",
    "#create a for loop to iterate through each column in the data\n",
    "for columns in cat_cols:\n",
    "    encode(df_KNN1[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%store inv_tran\n",
    "#del inv_tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['profile','Binder','Filler','Flavored','Has Tip','Origin','Pressed','Sweet','Wrapper', 'Brand:']\n",
    "\n",
    "categorical_columns = cat_cols\n",
    "for i,column in enumerate(categorical_columns):\n",
    "    label_list = inv_tran[i][0]\n",
    "    df[column] = [label_list[j] for j in df[column]]\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle('impute.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impute = pd.read_pickle('impute.pkl')\n",
    "df_impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verify = pd.concat([df_main, df_impute], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verify[['prod_name', 'Filler']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute data and convert \n",
    "encode_data = pd.DataFrame(np.round(imputer.fit_transform(df_KNN1)),columns = df_KNN1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute data and convert \n",
    "encode_data[list(cat_cols)] = encode_data[list(cat_cols)].astype(int)\n",
    "\n",
    "\n",
    "#encode_data.dtypes\n",
    "df = encode_data[cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.DataFrame(encoder.categories_)\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['rating'].fillna( 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['rating_count'].fillna( 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['Has Tip'].fillna( 'No', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_nobrand = df_main.drop('Brand:',1)\n",
    "df_nobrand['is_null'] = df_nobrand.isna().sum(axis=1).apply(lambda x: 0 if x==0 else 1)\n",
    "df_nobrand['is_null'].sum()\n",
    "df_main['is_null'] = df_main.isna().sum(axis=1).apply(lambda x: 0 if x==0 else 1)\n",
    "df_main['is_null'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nobrand = df_main.drop('Brand:',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nobrand['is_null'] = df_nobrand.isna().sum(axis=1).apply(lambda x: 0 if x==0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nobrand['is_null'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['is_null'] = df_main.isna().sum(axis=1).apply(lambda x: 0 if x==0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['is_null'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_main[df_main['profile'].isnull()]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_main['Binder'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = df_main\n",
    "# Here we use a column with categorical data\n",
    "fig = px.histogram(df, x=\"Binder\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_main.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_main['rating'] = [x.split(' ')[1] if x!=None else x for x in df_main['rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_main['rating_count'] = [x.split(' ')[3] if x!=None else x for x in df_main['rating_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[[\"prod_name\",\"desc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_process(desc):\n",
    "    \n",
    "    '''tokenize, lower, stopwords, remove punctuation, lemmatizer'''\n",
    "    \n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(desc)\n",
    "    #lower words\n",
    "    lower_words=[word2.lower() for word2 in tokens]\n",
    "    #remove stopwords\n",
    "    stp_words = ([word3 for word3 in lower_words if word3 not in (stopwords.words('english'))])\n",
    "    #remove punctuation\n",
    "    text_without_punct = [s.translate(str.maketrans('', '', string.punctuation)) for s in stp_words]\n",
    "    #lemmatize\n",
    "    lemmatized_words=[lemmatizer.lemmatize(word=word,pos='v') for word in text_without_punct]\n",
    "    lemmatizeddf= pd.DataFrame({'original_word': text_without_punct,'lemmatized_word': lemmatized_words})\n",
    "    lemmatizeddf=lemmatizeddf[['original_word','lemmatized_word']]\n",
    "    return lemmatizeddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_process(df_main['desc'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(row):\n",
    "    prod_name = df_main['prod_name'][row]\n",
    "    desc = df_main['desc'][row]\n",
    "    process = df_main['process_desc'][row]\n",
    "    return display(prod_name), display(desc), display(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize df\n",
    "\n",
    "#df_main['process_desc'] = df_main.apply(lambda row: nlp_process(row['desc']), axis=1)\n",
    "\n",
    "# df_main['desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize df\n",
    "\n",
    "#df_main['desc'] = df_main.apply(lambda row: nltk.word_tokenize(row['desc']), axis=1)\n",
    "\n",
    "# df_main['desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = nltk.word_tokenize(df_main['desc'][0])\n",
    "# token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lower\n",
    "\n",
    "# texts=token\n",
    "# lower_words=[word.lower() for word in texts]\n",
    "# lower_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stopwords\n",
    "\n",
    "# text = lower_words\n",
    "# stop_words = ([word for word in text if word not in (stopwords.words('english'))])\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(f\"Punctuation symbols: {string.punctuation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "# text_without_punct = [s.translate(str.maketrans('', '', string.punctuation)) for s in stop_words]\n",
    "# text_without_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
    "# display(f\"Text without punctuation: {text_without_punct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize trouble variations\n",
    "\n",
    "# words=text_without_punct\n",
    "# lemmatized_words=[lemmatizer.lemmatize(word=word,pos='v') for word in words]\n",
    "# lemmatizeddf= pd.DataFrame({'original_word': words,'lemmatized_word': lemmatized_words})\n",
    "# lemmatizeddf=lemmatizeddf[['original_word','lemmatized_word']]\n",
    "# lemmatizeddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = df_main['prod_name']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "tfid_df = pd.DataFrame(X)\n",
    "df_vect = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vect.drop([col for col, val in df_vect.mean().iteritems() if val < 0.0025], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vect.to_pickle('prod_vect.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. only keep words w average tfidf scores over 0.0001\n",
    "relevant = []\n",
    "for word in df_vect.columns:\n",
    "    if df_vect[word].mean() > 0.0025:\n",
    "        relevant.append(df_vect[word])\n",
    "len(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_main, df_vect]\n",
    "\n",
    "result = pd.concat(frames, axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[\"desc\"]= df_main[\"desc\"].astype(str) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_words = ['cigar', 'year', 'back', 'time', 'one', 'limited', 'cigars', 'new', 'get'\n",
    "                   ,'years', 'blend','good','La','Gurkha','like','made','Serie', 'classic', 'dark','name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add punctuation to stopwords_list\n",
    "stopwords_list+=string.punctuation\n",
    "## Add additional_words to stopwords_list\n",
    "stopwords_list.extend(additional_words)\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud Product Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordcloud = WordCloud(width=800, height=400, stopwords=stopwords_list, background_color=\"white\", max_words=200, \n",
    "                      contour_width=3, \n",
    "                      contour_color='steelblue')\n",
    "\n",
    "wordcloud.generate(df_main['desc'].to_string())\n",
    "\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data = df_main['desc']\n",
    "# data.head()\n",
    "\n",
    "tf_idf_vectorizor = TfidfVectorizer(stop_words = stopwords,#tokenizer = tokenize_and_stem,\n",
    "                             max_features = 5000)\n",
    "%time tf_idf = tf_idf_vectorizor.fit_transform(data)\n",
    "tf_idf_norm = normalize(tf_idf)\n",
    "tf_idf_array = tf_idf_norm.toarray()\n",
    "pd.DataFrame(tf_idf_array, columns=tf_idf_vectorizor.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_centroids = np.random.permutation(tf_idf_array.shape[0])[:3]\n",
    "initial_centroids\n",
    "centroids = tf_idf_array[initial_centroids]\n",
    "centroids.shape\n",
    "dist_to_centroid =  pairwise_distances(tf_idf_array,centroids, metric = 'euclidean')\n",
    "cluster_labels = np.argmin(dist_to_centroid, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    \"\"\" K Means Clustering\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "        k: int , number of clusters\n",
    "        \n",
    "        seed: int, will be randomly set if None\n",
    "        \n",
    "        max_iter: int, number of iterations to run algorithm, default: 200\n",
    "        \n",
    "    Attributes\n",
    "    -----------\n",
    "       centroids: array, k, number_features\n",
    "       \n",
    "       cluster_labels: label for each data point\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k, seed = None, max_iter = 200):\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "            \n",
    "    \n",
    "    def initialise_centroids(self, data):\n",
    "        \"\"\"Randomly Initialise Centroids\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        centroids: array of k centroids chosen as random data points \n",
    "        \"\"\"\n",
    "        \n",
    "        initial_centroids = np.random.permutation(data.shape[0])[:self.k]\n",
    "        self.centroids = data[initial_centroids]\n",
    "\n",
    "        return self.centroids\n",
    "    \n",
    "    \n",
    "    def assign_clusters(self, data):\n",
    "        \"\"\"Compute distance of data from clusters and assign data point\n",
    "           to closest cluster.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        cluster_labels: index which minmises the distance of data to each\n",
    "        cluster\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(-1, 1)\n",
    "        \n",
    "        dist_to_centroid =  pairwise_distances(data, self.centroids, metric = 'euclidean')\n",
    "        self.cluster_labels = np.argmin(dist_to_centroid, axis = 1)\n",
    "        \n",
    "        return  self.cluster_labels\n",
    "    \n",
    "    \n",
    "    def update_centroids(self, data):\n",
    "        \"\"\"Computes average of all data points in cluster and\n",
    "           assigns new centroids as average of data points\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        centroids: array, k, number_features\n",
    "        \"\"\"\n",
    "        \n",
    "        self.centroids = np.array([data[self.cluster_labels == i].mean(axis = 0) for i in range(self.k)])\n",
    "        \n",
    "        return self.centroids\n",
    "    \n",
    "    \n",
    "    def convergence_calculation(self):\n",
    "        \"\"\"\n",
    "        Calculates \n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict which cluster data point belongs to\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array or matrix, number_rows, number_features\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        cluster_labels: index which minmises the distance of data to each\n",
    "        cluster\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.assign_clusters(data)\n",
    "    \n",
    "    def fit_kmeans(self, data):\n",
    "        \"\"\"\n",
    "        This function contains the main loop to fit the algorithm\n",
    "        Implements initialise centroids and update_centroids\n",
    "        according to max_iter\n",
    "        -----------------------\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        instance of kmeans class\n",
    "            \n",
    "        \"\"\"\n",
    "        self.centroids = self.initialise_centroids(data)\n",
    "        \n",
    "        # Main kmeans loop\n",
    "        for iter in range(self.max_iter):\n",
    "\n",
    "            self.cluster_labels = self.assign_clusters(data)\n",
    "            self.centroids = self.update_centroids(data)          \n",
    "            if iter % 100 == 0:\n",
    "                print(\"Running Model Iteration %d \" %iter)\n",
    "        print(\"Model finished running\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# create blobs\n",
    "data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.6, random_state=50)\n",
    "# create np array for data points\n",
    "points = data[0]\n",
    "# create scatter plot\n",
    "plt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "\n",
    "X = data[0]\n",
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import string\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# email module has some useful functions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
